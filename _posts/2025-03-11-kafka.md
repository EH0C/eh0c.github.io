---
layout: post
title: Apache Kafka Data Streaming
subtitle: Real time data streaming using apache kafka!
tags: [test]
comments: true
mathjax: true
author: Edoardo Herianto
---

{: .box-success}
Here’s how I created a Real Time Data Streaming Application using Apache Kafka, Debezium, and Pyspark all in a dockerized environment.

![Crepe](/assets/img/kafka.png)

{: .box-note}
**Objective:** The goal is to capture real-time changes from a MariaDB database, stream them through Kafka topics, and process the data using PySpark for transformation, analysis, or further storage.

## Things I prepared beforehand:
- Installed Docker on my computer
- Download kafka-avro-serializer.jar
- Included the MySQL JDBC driver for database connectivity

## Docker Configuration:

My directory is structured like this:
~~~
my_project/
│-- docker-compose.yml
│-- pyspark/
│   ├── notebooks/  (Place your Jupyter notebooks here)
│   ├── requirements.txt  (Python dependencies)
│-- jars/  (Place your Avro/Protobuf JARs here)
~~~

My "compose.yaml" includes services for PySpark, Jupyter Notebook, Kafka, and Zookeeper
~~~
services:   

  mariadb:
    image: mariadb:10.3
    container_name: mariadb
    restart: always
    ports:
      - '3306:3306'
    environment:
      - MYSQL_ROOT_PASSWORD=passcword
      - MYSQL_USER=root
      - MYSQL_PASSWORD=password
    command: --log-bin=mysql-bin --binlog-format=ROW --server-id=1 --expire_logs_days=10
    volumes:
      - './mysql/dbdata:/var/lib/mysql'
      - './mysql/conf/my.cnf:/etc/mysql/conf.d/my.cnf'
      - './mysql/log:/var/log/mysql'

  zookeeper:
    image: wurstmeister/zookeeper:latest
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:5.5.1
    restart: always
    depends_on:
      - zookeeper
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT"
      KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092"
      KAFKA_LISTENER_NAME: PLAINTEXT
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9991
    ports:
      - "9092:9092"
    volumes:
      - D:/datastreamingcomp/kafka-avro-serializer-7.6.0.wso2v1.jar:/usr/share/java/kafka-avro-console-consumer.jar

  schema-registry:
    image: confluentinc/cp-schema-registry:5.5.3
    restart: always
    environment:
      - SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=zookeeper:2181
      - SCHEMA_REGISTRY_HOST_NAME=schema-registry
      - SCHEMA_REGISTRY_LISTENERS=http://schema-registry:8087,http://localhost:8087
    ports:
      - 8087:8087
    depends_on: [zookeeper, kafka]

  debezium:
    image: debezium/connect:1.4
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets
      STATUS_STORAGE_TOPIC: connect_status
      KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8087
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8087
      OFFSET_FLUSH_INTERVAL_MS: 60000
      OFFSET_FLUSH_TIMEOUT_MS: 5000
      SHUTDOWN_TIMEOUT: 10000
    ports:
      - 8083:8083
    depends_on:
      - kafka
      - schema-registry
    volumes:
      - D:\datastreamingcomp\kafka\config:/kafka/config
      - D:/datastreamingcomp/mysql-connector:/usr/share/java/kafka-connect

  pyspark:
    image: jupyter/pyspark-notebook:latest
    ports:
      - "8888:8888"
    environment:
      - PYSPARK_SUBMIT_ARGS=--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-avro_2.12:3.5.0 --jars /home/jovyan/work/mysql-connector-j-9.0.0.jar pyspark-shell
    volumes:
      - D:/mariakafka/pyspark:/home/jovyan/work
      - D:/mysql-connector-j-9.0.0.jar:/home/jovyan/work/mysql-connector-j-9.0.0.jar
    depends_on:
      - kafka

networks:
  default:
    driver: bridge
~~~

Start Docker Compose:
~~~
docker-compose up -d
~~~

## Creating a Database and Sample Data in the mariadb Container:

{: .box-warning}
**Warning:** Make sure to set binlog-format=ROW.

Here's what my table looks like:

| Number | Next number | Previous number |
| :------ |:--- | :--- |
| Five | Six | Four |
| Ten | Eleven | Nine |
| Seven | Eight | Six |
| Two | Three | One |

## Create a Debezium Connection:
Create a Debezium Connection
~~~
curl.exe -X POST -H "Content-Type: application/json" --data @D:\curls\mariadb.json http://localhost:8083/connectors
~~~
List all Connections to make sure the Debezium connection has been made
~~~
curl.exe -X GET http://localhost:8083/connectors
~~~
Check Connection Status
~~~
curl.exe -X GET http://localhost:8083/connectors/mariadb-connector/status
~~~
Check Connection Structure
~~~
curl.exe -X GET http://localhost:8087/subjects/mariadb.basecamp.employees-value/versions/latest
~~~
